**Overview**

***Answering questions with Data***
- Building a dataset:
    Does the dataset fit the problem?
    Is the dataset complete?
    How can you annotate a dataset and ensure the quality of the data and user experience over time?

Notes:
- How a model performs depends on the quality of the data that it is fed

***Data size***
- In the case of deep learning algorithms, data size becomes even more important
- Deep learning (neural networks) need to see many examples of every possible category before they can learn to distinguish between different classes of data and find general patterns in some data
- If you have too few data points or if the data is not evenly distributed between different categories that you want to distinguish, you can get some significant sampling bias in the end predictions
    Example: predictions that are biased towards classifying all data into one class, or predictions that have learned to find patterns that are irrelevant to the task at hand

***Data fit***
- Use production data (i.e., data generated by users) to ensure the training data matches real-world scenarios
- Determine the success criteria for a trained model:
    Precision
    Recall
    F1 score
    If not met --> re-train, go back to the data
- A confusion matrix can be used to calculate precision, recall, and f1 scores
    Precision: defined as the number of true positives over all predicted positives (i.e., true positives plus false positives)
    Recall: defined as the number of true positives over total actual positives (i.e., true positives plus false negatives)
    Accuracy: defined as the number of true positives plus true negatives divided by the total number of data points
    F1 score: 2 * (Precision * Recall) / (Precision + Recall)
    At its highest F1 = 1, at its lowest F1 = 0

***Data completeness***
- What is the problem you are trying to solve and how does it benefit end users?
- What data will help solve that problem?
    Collect data and observe relationships; patterns and similarities among the data
    Identify potential anomalies or missing data
- Conduct research and get the best data to serve the specific use case

***Data annotation***
- Datasets for training machine learning models often come in a tabular format (e.g., .csv spreadsheet)
- If a data source does not have any identifying labels or features (e.g., a set of images with no labels), a data annotation platform can be used
- A data annotation platform sends unlabeled data to human annotators who classify or provide features for the data and send it back in a tabular format
- Example data annotation platform services include AWS, Figure Eight, etc.
- The goal of data annotation is the get from unstructured, unlabeled data (e.g., text, video, image, audio) to a desired, labeled output
- Example data annotation jobs include:
    Sentiment analysis - contributors analyze text about a topic or brand
    Search relevance - contributors compare query and result pairings to help fine-tune algorithms
    Data categorization - contributors organize data
    Data collection & enrichment - contributors find business data, addresses, URLs, etc. based on what data needs to be collected
    Data validation - contributors look at the data and determine if its accurate
    Image annotation - contributors use annotation tools to mark images and highlight objects
    Transcription - contributors transcribe text from an image, PDF, audio file, etc.
    Content moderation - contributors make sure the content on the site is up to standards

***Summary***
- The underlying data determines the efficacy and accuracy of a model
- Data completeness and product fit are important considerations when using machine learning in a product
- The dataset can be built by designing a data annotation job
- Annotation jobs rely on human annotators that need clear instructions and examples
- Data annotations and / or model should be updated as needed according to changes in the underlying data
